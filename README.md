# Sentence-Encoder-Unum

In file My_dataset_script.py I create a my_data.csv file and put a 20 million sentence from bookcorpus dataset in it (all dataset(74 million) is too much), then in file tokenization_and_mask_in_train_loop.py I read the created file and datas in it.
